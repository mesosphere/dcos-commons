{
  "name": "Dcos-commons",
  "tagline": "The DC/OS Stateful Service SDK",
  "body": "# Overview\r\n\r\nMesos is a powerful resource allocator, but writing a production\r\nstateful service can be complex, tedious, and error-prone.  The DC/OS\r\nStateful Service SDK provides a high-level interface for\r\nwriting stateful services.  Now, you can write a stateful service\r\ncomplete with\r\npersistent volumes, fault tolerance, and configuration management in\r\nabout 100 lines of code.  This SDK is the product of Mesosphere's\r\nexperience writing production stateful services such as [DC/OS Apache\r\nKafka](https://docs.mesosphere.com/latest/usage/service-guides/kafka/)\r\n, [DC/OS Apache\r\nCassandra](https://docs.mesosphere.com/latest/usage/service-guides/\r\ncassandra/), and [DC/OS Apache\r\nHDFS](https://docs.mesosphere.com/latest/usage/service-guides/hdfs/).\r\n\r\nRead about the [features](#Features-at-a-glance) to learn more about\r\nthe DC/OS Stateful Service SDK.\r\n\r\n# Introduction\r\n\r\nIn this tutorial, we'll build an example\r\nservice called `data-store`.  It is composed of a small number of\r\n`meta-data` nodes and a larger number of `data` nodes.  This structure\r\nis meant to model a service with a small number of coordinating master\r\nnodes and a large number of dependent agent nodes.  Each task runs a\r\ncommand that writes some data to a file, then sleeps.\r\n\r\nThe goal state for this fictional `data-store` is to keep two\r\n`meta-data` and N `data` nodes running, where, initially, N = 3.\r\n`meta-data` nodes must come up before `data` nodes.  The service\r\nscales by starting more `data` nodes.\r\n\r\nThis tutorial includes code snippets. The final\r\nsource code is available at\r\n[http://github.com/mesosphere/sdk-reference-framework](http://github.\r\ncom/mesosphere/sdk-reference-framework).\r\n\r\n## Architecture\r\n\r\n![DC/OS Service Architecture](./architecture.png)\r\n\r\nA DC/OS service is composed of **schedulers** and **executors**.  The\r\nscheduler is run by Marathon, which supervises the scheduler,\r\nrestarting it upon failure.  The scheduler, in turn, starts and\r\nsupervises the executors.  If specified in the `ServiceSpecification`,\r\nthe\r\nexecutors will contain a **persistent volume**, which is a folder or\r\nmount point on the agent that persists even after a task fails. This\r\nallows a stateful service to recover with its data intactafter task\r\nfailure or temporary node failure.\r\n\r\nIn this tutorial, `data-store` is the scheduler, and `meta-data-{0,1}`\r\nand `data-{0-1}` are the executors.\r\n\r\n## Requirements\r\n- JDK 8\r\n- Gradle 3.1 (recommended)\r\n- AWS account with S3 upload permissions\r\n- [`aws` CLI](https://aws.amazon.com/cli/)\r\n\r\n# Create a Project\r\n\r\nThe Java library for The DC/OS Stateful Services SDK is\r\n`dcos-commons`.  Java projects using `dcos-commons` can use any Java\r\nbuild system, but for this tutorial, we'll assume you're using Gradle.\r\n\r\nFirst, we create our `build.gradle` file:\r\n\r\n```\r\ngroup 'com.mesosphere.sdk'\r\nversion '1.0-SNAPSHOT'\r\n\r\napply plugin: 'java'\r\napply plugin: 'application'\r\n\r\nrepositories {\r\n    mavenLocal()\r\n    mavenCentral()\r\n    maven {\r\n        url \"http://downloads.mesosphere.com/maven/\"\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile \"mesosphere:dcos-commons:0.7.10-SNAPSHOT\"\r\n    compile \"org.slf4j:slf4j-simple:1.7.21\"\r\n}\r\n\r\ntask wrapper(type: Wrapper) {\r\n    gradleVersion = '3.1'\r\n}\r\n\r\nmainClassName = 'com.mesosphere.sdk.reference.scheduler.Main'\r\n```\r\n\r\n# Define a Service\r\n\r\n## Create a Service Specification\r\n\r\nThe first step to running a service is to create a\r\n`ServiceSpecification`, which defines the behavior of the service.\r\n\r\nA `ServiceSpecification` is mostly a list of `TaskSet`s.\r\nThe tasks defined in this list are launched in order.  We can use this\r\nordering to define tiers of services.  In this example, we want all\r\n`meta-data` nodes to come up before any `data` node:\r\n\r\n```java\r\nprivate static final String TASK_METADATA_NAME = \"meta-data\";\r\nprivate static final int TASK_METADATA_COUNT = Integer.valueOf(System.getenv(\"METADATA_COUNT\"));\r\nprivate static final double TASK_METADATA_CPU = Double.valueOf(System.getenv(\"METADATA_CPU\"));\r\nprivate static final double TASK_METADATA_MEM_MB = Double.valueOf(System.getenv(\"METADATA_MEM\"));\r\nprivate static final double TASK_METADATA_DISK_MB = Double.valueOf(System.getenv(\"METADATA_DISK\"));\r\nprivate static final String TASK_METADATA_URI = System.getenv(\"METADATA_URI\");\r\nprivate static final String TASK_METADATA_CMD = \"bash meta-data\";\r\n\r\nprivate static final String TASK_DATA_NAME = \"data\";\r\nprivate static final int TASK_DATA_COUNT = Integer.valueOf(System.getenv(\"DATA_COUNT\"));\r\nprivate static final double TASK_DATA_CPU = Double.valueOf(System.getenv(\"DATA_CPU\"));\r\nprivate static final double TASK_DATA_MEM_MB = Double.valueOf(System.getenv(\"DATA_MEM\"));\r\nprivate static final double TASK_DATA_DISK_MB = Double.valueOf(System.getenv(\"DATA_DISK\"));\r\nprivate static final String TASK_DATA_URI = System.getenv(\"DATA_URI\");\r\nprivate static final String TASK_DATA_CMD = \"bash data\";\r\n\r\nprivate static ServiceSpecification getServiceSpecification() {\r\n    return new ServiceSpecification() {\r\n        @Override\r\n        public String getName() {\r\n            return SERVICE_NAME;\r\n        }\r\n\r\n        @Override\r\n        public List<TaskSet> getTaskSets() {\r\n            return Arrays.asList(\r\n                    DefaultTaskSet.create(\r\n                            TASK_METADATA_COUNT,\r\n                            TASK_METADATA_NAME,\r\n                            getCommand(TASK_METADATA_CMD, TASK_METADATA_URI),\r\n                            getResources(TASK_METADATA_CPU, TASK_METADATA_MEM_MB),\r\n                            getVolumes(TASK_METADATA_DISK_MB, TASK_METADATA_NAME),\r\n                            Optional.of(TaskTypeGenerator.createAvoid(TASK_METADATA_NAME)),\r\n                            Optional.of(getHealthCheck(TASK_METADATA_NAME))),\r\n                    DefaultTaskSet.create(\r\n                            TASK_DATA_COUNT,\r\n                            TASK_DATA_NAME,\r\n                            getCommand(TASK_DATA_CMD, TASK_DATA_URI),\r\n                            getResources(TASK_DATA_CPU, TASK_DATA_MEM_MB),\r\n                            getVolumes(TASK_DATA_DISK_MB, TASK_DATA_NAME),\r\n                            Optional.of(TaskTypeGenerator.createAvoid(TASK_DATA_NAME)),\r\n                            Optional.of(getHealthCheck(TASK_DATA_NAME))));\r\n        }\r\n    };\r\n}\r\n```\r\n\r\nA `TaskSet` contains the bulk of the definition.  It\r\ncontains the command to run, the resources to run with, and the number\r\nof tasks.  As you can see, it is common for DC/OS services to read\r\ntheir config from environment variables.\r\n\r\nAt a minimum, the command must contain a `value`, which is the shell\r\ncommand to invoke.  It will likely also contain one or more `URIs`\r\npointing to resources to download before execution.  In this example,\r\nwe're downloading the `metadata` and `data` binaries from an HTTP\r\nserver:\r\n\r\n```java\r\nprivate static Protos.CommandInfo getCommand(String cmd, String uri) {\r\n    return Protos.CommandInfo.newBuilder()\r\n              .addUris(Protos.CommandInfo.URI.newBuilder().setValue(uri))\r\n              .setValue(cmd)\r\n              .build();\r\n}\r\n```\r\n\r\nResurces are specified via `ResourceSpecification`s.  Tasks must run\r\nwith at least `cpus` and `mem` resources, though we may specify any\r\nMesos resource, including `ports`:\r\n\r\n```java\r\nprivate static Collection<ResourceSpecification> getResources(double cpu, double memMb) {\r\n    return Arrays.asList(\r\n            new DefaultResourceSpecification(\r\n                    \"cpus\",\r\n                    ValueUtils.getValue(ResourceUtils.getUnreservedScalar(\"cpus\", cpu)),\r\n                    ROLE,\r\n                    PRINCIPAL),\r\n            new DefaultResourceSpecification(\r\n                    \"mem\",\r\n                    ValueUtils.getValue(ResourceUtils.getUnreservedScalar(\"mem\", memMb)),\r\n                    ROLE,\r\n                    PRINCIPAL));\r\n}\r\n```\r\n\r\nIn Mesos, `disk` is just another resource, like `cpus` and `mem`.\r\nHowever, stateful services must manage `disk` differently.  Rather\r\nthan simply accepting the resource like `cpus` or `mem`, the scheduler\r\nmust first create a\r\n[volume](http://mesos.apache.org/documentation/latest/persistent-\r\nvolume/)\r\nfrom the disk resources so that it persists beyond the lifetime of the\r\ntask.  The `VolumeSpecification` is a high-level interface that\r\nhandles this for us:\r\n\r\n```java\r\nprivate static Collection<VolumeSpecification> getVolumes(double diskMb, String taskName) {\r\n    VolumeSpecification volumeSpecification = new DefaultVolumeSpecification(\r\n            diskMb,\r\n            VolumeSpecification.Type.ROOT,\r\n            taskName + CONTAINER_PATH_SUFFIX,\r\n            ROLE,\r\n            PRINCIPAL);\r\n\r\n    return Arrays.asList(volumeSpecification);\r\n}\r\n```\r\n\r\nFinally, we can attach a health check to our tasks.  If the check\r\nfails, the Mesos agent will kill the task, and the scheduler will\r\nrestart it.\r\n\r\n```java\r\nprivate static Protos.HealthCheck getHealthCheck(String name) {\r\n    Protos.CommandInfo commandInfo = Protos.CommandInfo.newBuilder()\r\n        .setValue(\"stat %s%s/output\".format(name, CONTAINER_PATH_SUFFIX))\r\n        .build()\r\n\r\n    return Protos.HealthCheck.newBuilder()\r\n        .setCommand(commandInfo)\r\n        .build();\r\n}\r\n```\r\n\r\n## Define a Main Class\r\n\r\nNow that we have a `ServiceSpecification`, we must define a Main class\r\nthat runs it.  To do this, create a `Service` object, and register the\r\n`ServiceSpecification` with it:\r\n\r\n```java\r\npublic class Main {\r\n    private static final int API_PORT = Integer.valueOf(System.getenv(\"PORT0\"));\r\n    public static void main(String[] args) throws Exception {\r\n        new DefaultService(API_PORT).register(getServiceSpecification());\r\n    }\r\n}\r\n```\r\n\r\n`DefaultService` takes a single parameter, `apiPort`. `apiPort`\r\nspecifies which\r\nport its HTTP server will bind to.  Upon registering the\r\n`ServiceSpecification`, our service will start its HTTP server,\r\nregister as a Mesos framework, and start launching tasks.\r\n\r\n# Deployment\r\n\r\nNow that we've defined our service, let's build and install it.  The\r\nfirst thing we'll need is a DC/OS cluster.\r\n\r\n## Start cluster\r\n\r\nGo to [https://dcos.io/install/](https://dcos.io/install/) to install\r\na DC/OS cluster.\r\n\r\n## Install the DC/OS CLI\r\n\r\nIf you have not already installed the CLI, do so by following [these\r\ninstructions](https://dcos.io/docs/latest/usage/cli/install/).\r\n\r\n## Install\r\n\r\nNow that we have our cluster and CLI, we can install our service.\r\nDC/OS services are deployed to users via DC/OS packages.  The first\r\nstep is to define our package.\r\n\r\n### Define a DC/OS Package\r\n\r\nA DC/OS package is a set of four\r\nfiles.  You can learn more about the package format\r\n[here](http://github.com/mesosphere/universe), but, in summary, it\r\nlooks like this:\r\n\r\n- **package.json**: Package metadata\r\n- **marathon.json.mustache**: Marathon app definition.  The template\r\nis rendered with the values in config.json, resource.json,\r\nand user-provided config.\r\n- **config.json**: JSON Schema defining the configuration for the app.\r\n- **resource.json**: External resources.\r\n\r\nThe majority of the package definition resides in\r\n**marathon.json.mustache**.  See the\r\n[reference\r\nframework](https://github.com/mesosphere/sdk-reference-framework/tree/\r\nmaster/universe)\r\nfor an example.\r\n\r\nLet's place these files in a directory in our project called\r\n`universe/`.\r\n\r\nNow we build our project:\r\n\r\n```bash\r\n$ ./gradlew distZip\r\n```\r\n\r\n### Build a DC/OS Package\r\n\r\nThen we must build our DC/OS package.  DC/OS packages are stored in\r\n*repos*.  The most common repo is the\r\n[*Mesosphere Universe*](https://github.com/mesosphere/universe), which\r\nis public. We would publish our package there if we wished to\r\nshare it with the world.  For now, we just want to create a simple\r\nrepo containing our single package.\r\n\r\n**NOTE:** The current workflow of creating a package *and* a repo is\r\n  cumbersome.  We're sorry about this.  DC/OS 1.10, which is slated to\r\n  be released in December 2016, will introduce the ability to install\r\n  a package directly, rather than forcing you to wrap it in a repo.\r\n\r\n```bash\r\n~ $ cd /path/to/dcos-commons\r\ndcos-commons $ ./tools/ci_upload.py \\\r\n      data-store \\\r\n      universe/ \\\r\n      reference-scheduler/build/distributions/scheduler.zip\r\n```\r\n\r\n`./ci_upload.py` uploads the artifact to S3, rewrites the package to\r\ninclude links to the artifact, creates a repo containing that package,\r\nand uploads that repo to S3.  The end result is a URL for a DC/OS\r\nrepo.\r\n\r\nWe now add this repo to our cluster:\r\n\r\n```bash\r\n$ dcos package repo add --index=0 dev-repo <repo_url>\r\n```\r\n\r\nThen, install our package:\r\n\r\n```bash\r\n$ dcos package install data-store\r\n```\r\n\r\nVisit **Services** > **data-store** in the DC/OS UI to watch\r\n`data-store` deploying.\r\n\r\nTo verify that `data-store` has completed deploying, make the\r\nfollowing HTTP request from the DC/OS CLI. To learn how to make\r\nrequests to the HTTP API,\r\nplease read the [HTTP API](#http-api) section:\r\n\r\n```\r\nGET /v1/plan/status\r\n```\r\n\r\nThis HTTP request will return JSON to indicate the status of the\r\nservice. If the deployment is complete, `plan.status` will equal\r\n\"COMPLETE\".\r\n\r\nWhen deployment is complete, two `meta-data`\r\ntasks and three `data` tasks will be visible in the DC/OS Services UI.\r\n\r\n## Uninstall\r\n\r\nTo do a complete uninstall, we first uninstall our service using the\r\nDC/OS CLI.  This stops and tears down the framework.\r\n\r\n```bash\r\n$ dcos package uninstall data-store\r\n```\r\n\r\nHowever, uninstalling the package does not remove persistent state\r\nlike Zookeeper state and persistent volumes. Mesosphere provides a\r\nDocker image to remove persistent state:\r\n\r\n```bash\r\nlocal $ AUTH_TOKEN=$(dcos config show core.dcos_acs_token)\r\nlocal $ dcos node ssh --master-proxy --leader\r\ncluster $ docker run mesosphere/janitor /janitor.py \\\r\n    -r data-store-role \\\r\n    -p data-store-principal \\\r\n    -z dcos-service-data-store \\\r\n    --auth_token=$AUTH_TOKEN\r\n```\r\n\r\nFor more information on the janitor image, see\r\nhttps://github.com/mesosphere/framework-cleaner.\r\n\r\n\r\n# Plan management\r\nAs noted earlier, stateful services often have particular deployment\r\nrequirements.  These deployments may involve the initial creation of\r\nTasks or a change in Task configuration or the scaling of a\r\nparticular kind of task or an upgrade to a new version of a Task.  Each\r\nof these scenarios comes with attendant risks and benefits.  In some\r\ncases (as in initial install) you may be happy to have a service rollout\r\nwithout intervention.  In others, as in a configuration update, one may\r\nwish to deploy some portion of a service and check stability before\r\ncompleting a deployment.\r\n\r\nIn order to provide the necessary flexibility and control necessary to\r\nachieve these outcomes, the SDK organizes deployment of tasks by means\r\nof Plans. Plans are composed of Phases which are composed of Blocks.\r\nA Block encapsulates the smallest element of work executed in a Plan and\r\nis the plan element which encapsulates the launching of Mesos Tasks.\r\n\r\nPhases are groups of Blocks which have some semantic relationship.  At\r\nleast two plans are generated automatically by the SDK when presented\r\nwith a `ServiceSpecification`.  These are the deployment and recovery\r\nplans.  The deployment plan is concerned with deploying Tasks.  It\r\ndeploys Tasks for the first time, and when configuration updates are\r\nindicated by a user.  The recovery plan is concerned with defining the\r\nnecessary operations which must be performed when transient or permanent\r\nfailures are encountered.\r\n\r\nLet us examine the deployment plan of the example data-store service\r\nafter its initial installation has completed.\r\n\r\nExecuting the following command:\r\n\r\n```\r\nGET /v1/plan\"\r\n```\r\n\r\ngenerates the following output:\r\n\r\n```json\r\n{\r\n\t\"phases\": [{\r\n\t\t\"id\": \"31a3ca99-bdbd-45dd-a8f0-cc93feade61e\",\r\n\t\t\"name\": \"meta-data\",\r\n\t\t\"blocks\": [{\r\n\t\t\t\"id\": \"2a0db529-095b-4b63-8c56-075aa442c898\",\r\n\t\t\t\"status\": \"COMPLETE\",\r\n\t\t\t\"name\": \"meta-data-0\",\r\n\t\t\t\"message\": \"Block: meta-data-0\",\r\n\t\t\t\"has_decision_point\": false\r\n\t\t}, {\r\n\t\t\t\"id\": \"99210582-cdfb-4b63-95e9-19600f117dc7\",\r\n\t\t\t\"status\": \"COMPLETE\",\r\n\t\t\t\"name\": \"meta-data-1\",\r\n\t\t\t\"message\": \"Block: meta-data-1\",\r\n\t\t\t\"has_decision_point\": false\r\n\t\t}],\r\n\t\t\"status\": \"COMPLETE\"\r\n\t}, {\r\n\t\t\"id\": \"927ac279-b969-4106-9089-40264ae15dc4\",\r\n\t\t\"name\": \"data\",\r\n\t\t\"blocks\": [{\r\n\t\t\t\"id\": \"efcf9dc0-488f-4f9e-ae0c-1ac16c401e0a\",\r\n\t\t\t\"status\": \"COMPLETE\",\r\n\t\t\t\"name\": \"data-0\",\r\n\t\t\t\"message\": \"Block: data-0\",\r\n\t\t\t\"has_decision_point\": false\r\n\t\t}, {\r\n\t\t\t\"id\": \"a0e0905b-d4da-4cec-84b9-461a60a92818\",\r\n\t\t\t\"status\": \"COMPLETE\",\r\n\t\t\t\"name\": \"data-1\",\r\n\t\t\t\"message\": \"Block: data-1\",\r\n\t\t\t\"has_decision_point\": false\r\n\t\t}, {\r\n\t\t\t\"id\": \"fa85a843-e6eb-4074-b353-875197a18adb\",\r\n\t\t\t\"status\": \"COMPLETE\",\r\n\t\t\t\"name\": \"data-2\",\r\n\t\t\t\"message\": \"Block: data-2\",\r\n\t\t\t\"has_decision_point\": false\r\n\t\t}],\r\n\t\t\"status\": \"COMPLETE\"\r\n\t}],\r\n\t\"errors\": [],\r\n\t\"status\": \"COMPLETE\"\r\n}\r\n```\r\n\r\nThe first thing to notice is that the `TaskSet`s defined in a\r\n`ServiceSpecification` are mapped to deployment Phases. The data-store\r\nexample service has `TaskSet`s encapsulating `meta-data` and `data`\r\nTasks in that order.  The deployment plan above has the respective\r\n`meta-data` and `data` Phases.\r\n\r\nIn its default configuration, the two `data` Tasks and three `meta-data`\r\nTasks are launched.  We can see that a Block has been created\r\nencapsulating each of those tasks.  The names of Blocks map to the names\r\nof the `TaskSpecification` objects within a `TaskSet`.  The status of a\r\nBlock can be in one of 3 states.  It may be `PENDING`, `IN_PROGRESS`, or\r\n`COMPLETE`.\r\n\r\nBy default all Phases and the Blocks within them are rolled out\r\nserially.  Each Block must reach a `COMPLETE` state before the\r\nnext Block may be started.  Beyond this automatic behavior, it is often\r\ndesirable to manually pause/resume a deployment.  For example, in a\r\nproduction setting multiple uncoordinated deployments may cause\r\nunexpected effects, including causing unavailability or severely\r\ndegraded performance.  In thes cases and others, the ability to pause\r\nindividual deployments while others continue without performing a full\r\nrollback can be helpful.\r\n\r\nThe `has_decision_point` fields above indicate whether a user has indicated\r\na desire to pause a deployment at a particular Block.\r\n\r\nA Plan's execution may be paused or continued by POST commands executed\r\nagainst the HTTP endpoints at the [`/v1/plan/interrupt`](https://github.com/mesosphere/dcos-commons/blob/master/src/main/java/org/apache/mesos/scheduler/plan/api/PlanResource.java#L53)\r\nand [`/v1/plan/continue`](https://github.com/mesosphere/dcos-commons/blob/master/src/main/java/org/apache/mesos/scheduler/plan/api/PlanResource.java#L46)\r\nrespectively.  For example:\r\n\r\n```\r\nPOST /v1/plan/interrupt\"\r\n```\r\n\r\n```\r\nPOST /v1/plan/continue\"\r\n```\r\n\r\nFinally, beyond manual control of Plan execution behavior it may be\r\ndesirable in certain failure scenarios to either restart the execution\r\nof a Block or force its completion.  Restarting a Block in default\r\nimplementations simply sets a Block's status to `PENDING` and allows the\r\nnormal Block processing system to do the necessary work to drive a Block\r\nto completion.  This can be accomplished by issuing a POST command to\r\nthe [`/v1/plan/restart`](https://github.com/mesosphere/dcos-commons/blob/master/src/main/java/org/apache/mesos/scheduler/plan/api/PlanResource.java#L69)\r\nendpoint.  For example to restart the Block associated with the first\r\ndata task `data-0` one would issue the following command:\r\n\r\n```bash\r\nPOST /v1/plan/restart?phase=927ac279-b969-4106-9089-40264ae15dc4&block=efcf9dc0-488f-4f9e-ae0c-1ac16c401e0a\"\r\n```\r\n\r\nForcing the completion of a block can be accomplished in a similar\r\nmanner by issuing a POST command against the [`/v1/plan/forceComplete`](https://github.com/mesosphere/dcos-commons/blob/master/src/main/java/org/apache/mesos/scheduler/plan/api/PlanResource.java#L69)\r\nendpoint. For example to force the completion of the second `meta-data`\r\nblock one would issue the following command:\r\n\r\n```bash\r\nPOST /v1/plan/forceComplete?phase=31a3ca99-bdbd-45dd-a8f0-cc93feade61&block=99210582-cdfb-4b63-95e9-19600f117dc7\"\r\n```\r\n\r\n# Update configuration\r\n\r\nWe saw in the [Service Specification\r\nsection](#create-a-service-specification) that the service's\r\nconfiguration is read in through environment variables.  You can use\r\nany source of configuration, but DC/OS Services are typically\r\nconfigured through environment variables.  To make a configuration\r\nchange, modify the corresponding environment variable,\r\nthen restart the scheduler.  When the scheduler comes back up, it will\r\nread its new configuration and transition to the new state.\r\n\r\nFor example, in order to scale the `data-store` service from 3 `data`\r\nnodes to 5, modify the `DATA_COUNT` environment variable. To modify\r\nit:\r\n\r\n1) Go the the **Services** > **data-store** page of the DC/OS UI.\r\n\r\n1) Click **Edit**.\r\n\r\n1) Click the **Environment Variables** tab.\r\n\r\n1) Change the value of `DATA_COUNT` from 3 to 5.\r\n\r\n1) Click **Deploy Changes**.\r\n\r\nThis will restart the scheduler with the updated environment.  When\r\nthe scheduler starts up, it detects that it is already running 3\r\n`data` tasks and starts 2 more in order to reach the goal state of 5.\r\nWe should observe two more tasks starting, `data-3` and `data-4`.\r\n\r\n## Task Configuration management\r\n\r\nWhile the above describes configuration of the scheduler itself via\r\nenvironment variables, there's also a need for configuration of the\r\nunderlying service tasks themselves in a flexible way.\r\n\r\nTo simplify the common task of getting user-facing configuration to\r\nservice tasks, the developer may follow the following convention in\r\nnaming the environment variables for these configuration options:\r\n`TASKCFG_<TASK_TYPE>_<CFGNAME>`, where `<TASK_TYPE>` has been\r\nconverted from the task type to fit the requirements of environment\r\nvariables:\r\n\r\n- Uppercased\r\n- Non-alphanumeric characters (punctuation, whitespace) converted\r\n  to underscores\r\n\r\nFor example, an option named `FOO` for tasks of type `index.mgr`\r\nshould be named `TASKCFG_INDEX_MGR_FOO`, while an option `BAR` for\r\ntasks of type `data-node` should be named `TASKCFG_DATA_NODE_BAR`.\r\nThese configuration options will automatically be forwarded to the\r\nenvironments of the matching tasks as environment variables, with\r\nthe `TASKCFG_<TASK_TYPE>_` prefixes removed. A special prefix of\r\n`TASKCFG_ALL_<NAME>` may be used for any options that should be\r\npassed to *every* task type.\r\n\r\nA common need for service developers is an easy way to write\r\nconfiguration files before launching tasks. To fulfill this need,\r\nthe developer may provide configuration file template(s) in their\r\nTaskSet(s). These templates follow the [mustache](https://mustache.github.io/)\r\ntemplating format, similar to what's used in DC/OS packaging. The\r\ntemplates will be automatically rendered against the task's\r\nenvironment (which is customized as described above), and then\r\neach written to relative file paths specified by the developer.\r\n\r\n# Restart tasks\r\n\r\nWhen a task fails, the scheduler will attempt to restart the on the\r\nsame node, using the same persistent volume as the previous task.\r\n\r\nThere are two cases where you must manually restart tasks:\r\n\r\n1) If an agent permanently fails or is ailing, the scheduler will\r\nnever receive an\r\noffer with the expected persistent volume, and will thus never\r\nautomatically restart the task.  We must restart it manually on a\r\ndifferent node.  This is called *replacing* the task.\r\n\r\n2) If a task is stuck or performing poorly, or we just want to restart\r\nit for debugging, we may need to restart the task manually on the same\r\nnode.\r\n\r\nYou can perform both actions using the same endpoint:\r\n\r\n```\r\nPOST /v1/tasks/restart/<task-name>?replace={true|false}\r\n```\r\n\r\nIf `replace=true`, this action will replace the task on a separate\r\nnode.  If `replace=false`, it will restart the task on the same node.\r\n\r\n# Placement\r\n\r\nThe `DefaultScheduler` makes no intelligent placement decisions.  It\r\nwill run a task on first offer it gets that satisfies the resource\r\nconstraints for that task.\r\n\r\nPlacement decisions may be customized using *Placement Constraints*,\r\nwhich\r\nare rules that specify how tasks should be placed in the cluster.\r\nThese rules\r\nare enforced by filtering the resource offers returned by Mesos.\r\nResources that do not conform to the rules are removed from the list.\r\n\r\nYou can define placement constraints on a\r\nper-task basis by customizing the return value of\r\n`TaskSpecification.getPlacement()`:\r\n\r\n```\r\npublic Optional<PlacementRuleGenerator> getPlacement() {\r\n    // avoid systems which are running an \"index\" task:\r\n    return Optional.of(TaskTypeGenerator.createAvoid(\"index\"));\r\n}\r\n```\r\n\r\nYou can use some of the common `PlacementRule`s\r\nand/or `PlacementRuleGenerator` implementations provided in the\r\n[offer.constrain](https://github.com/mesosphere/dcos-commons/tree/\r\nmaster/src/main/java/org/apache/mesos/offer/constrain)\r\npackage. You can also create your own `PlacementRule`s for custom\r\nconstraints. Custom constraints can take advantage of anything present\r\nin the Mesos `Offers`\r\nor in the other running `TaskInfo`s.\r\n\r\n# Logs\r\n\r\nThe `stdout` and `stderr` streams for all tasks running in Mesos,\r\nincluding the scheduler and executors, are captured and written to\r\nfiles in that task's sandbox. You can view these files from the\r\n**Services** tab of the DC/OS UI or from the CLI. [Learn\r\nmore](https://docs.mesosphere.com/1.8/administration/logging/service-\r\nlogs/).\r\n\r\nAll code in `dcos-commons`, including the `DefaultScheduler`, uses\r\n`slf4j` to write logs to `stderr`.  To write your own logging code,\r\nfirst create a `LOGGER`:\r\n\r\n```java\r\nprivate static final Logger LOGGER = LoggerFactory.getLogger(Main.class);\r\n```\r\n\r\nThen write to it:\r\n\r\n```java\r\nLOGGER.info(\"Starting reference scheduler with args: \" + Arrays.asList(args));\r\n```\r\n\r\nThe contents of `stdout` and `stderr` for the executors is entirely\r\ndependent on the command specified in our `ServiceSpecification`.\r\n\r\n# Tests\r\n\r\n## Unit Tests\r\n\r\n`dcos-commons` contains unit\r\ntests written with `junit` and `mockito`. You can also create your own\r\nunit tests.\r\n\r\n## Integration Tests\r\n\r\nThe [shakedown](https://github.com/dcos/shakedown) library is a tool for\r\nwriting integration tests for DC/OS services.  For an example, see\r\n[the tests for DC/OS\r\nKafka](https://github.com/mesosphere/dcos-kafka-service/tree/master/integration/tests).\r\n\r\n# Metrics\r\n\r\nAs of DC/OS 1.8, this section is only relevant to DC/OS Enterprise\r\nEdition, but support has recently been\r\n[open-sourced](http://github.com/dcos/dcos-metrics)\r\nand should soon be available in Open DC/OS. For more information,\r\nsee the [dcos-metrics](http://github.com/dcos/dcos-metrics)\r\nrepository and stop by the #day2ops channel on [DC/OS\r\nSlack](https://chat.dcos.io).\r\n\r\nDC/OS Metrics automatically provides all Mesos containers with a\r\nunique UDP endpoint for outputting `statsd`-formatted metrics. The\r\nendpoint\r\nis advertised via two container environment variables:\r\n`STATSD_UDP_HOST`\r\nand `STATSD_UDP_PORT`. Any metrics sent to this advertised endpoint\r\nwill automatically be tagged with the originating container and\r\nforwarded upstream to the cluster's metrics infrastructure.\r\n\r\nYou can take advantage of this endpoint for your service\r\nscheduler as well as for the underlying service tasks themselves. For\r\ninstance, you can configure your service's tasks to emit to the\r\nlocally-advertised endpoint as they are launched.\r\n\r\n**Note:** The environment-advertised endpoint is\r\nunique to each container and cannot be reused across containers.\r\n\r\nThe data sent to this endpoint should follow the standard `statsd`\r\ntext format, optionally with multiple newline-separated values\r\nper UDP packet.\r\n[http://docs.datadoghq.com/guides/dogstatsd/#datagram-format](Datadog-\r\nextension)\r\ntags are also supported, so the application may also include its own\r\ncustom tags:\r\n\r\n```\r\nmemory.usage_mb:5|g\r\nfrontend.query.latency_ms:46|g|#shard_id:6,section:frontpage\r\n```\r\n\r\nSee also:\r\n- [dcos-metrics repo](https://github.com/dcos/dcos-metrics)\r\n- [Sample StatsD emitter\r\nprocess](https://github.com/dcos/dcos-metrics/tree/master/examples/\r\nstatsd-emitter)\r\n- [Metrics configuration for\r\nKafka](https://github.com/mesosphere/dcos-kafka-service/blob/\r\n30acc60676ba9362ddb9b74f208b36d257a78f93/kafka-config-overrider/src/\r\nmain/java/com/mesosphere/dcos/kafka/config/Overrider.java#L163)\r\n- [Metrics configuration for\r\nCassandra](https://github.com/mesosphere/dcos-cassandra-service/blob/\r\n38360f9f78d7063824ad77f9871108fe5609e54d/cassandra-executor/src/main/\r\njava/com/mesosphere/dcos/cassandra/executor/metrics/MetricsConfig.java\r\n#L68)\r\n\r\n# Service Discovery\r\n\r\nMesos-DNS assigns every Mesos task a DNS address of the form\r\n`<task-name>.<framework-name>.mesos`.  In our example (running 3 data\r\nnodes), Mesos-DNS creates the following addresses:\r\n\r\n```\r\nmeta-data-0.data-store.mesos\r\nmeta-data-1.data-store.mesos\r\nmeta-data-2.data-store.mesos\r\ndata-0.data-store.mesos\r\ndata-1.data-store.mesos\r\n```\r\n\r\nIt also creates a DNS entry for the scheduler:\r\n\r\n```\r\ndata-store.marathon.mesos\r\n```\r\n\r\nIf the tasks listen on reserved port(s), clients will also need\r\nthese port(s) in order to establish a connection.  We can fetch these\r\nport(s) along with the DNS entry for a task via the following\r\nendpoint:\r\n\r\n```bash\r\nGET /v1/tasks/connection/{task-name}\r\n```\r\n\r\nThis HTTP request returns:\r\n\r\n```json\r\n{\r\n  \"dns\": \"data-0.data-store.mesos\",\r\n  \"ports\": \"4388\"\r\n}\r\n```\r\n\r\nIf the task has multiple ports reserved, they will be displayed as a\r\ncomma-delimited sequence of ranges. e.g. `\"8080,2000-3000\"`.\r\n\r\n# Secrets Management\r\n\r\nThis section is only relevant to DC/OS Enterprise Edition.\r\n\r\nIn a DC/OS Enterprise Edition cluster running in strict security mode,\r\nall schedulers must authenticate with the Mesos master using DC/OS\r\nservice accounts.  For instructions on creating a service account,\r\nread\r\n[this][https://docs.mesosphere.com/1.8/administration/id-and-access-\r\nmgt/service-auth/]\r\n\r\nMesos schedulers authenticate to the master by providing a `principal`\r\nand `secret`.  In DC/OS, the `principal` must be the DC/OS Service\r\nAccount ID, and the `secret` must be the private key.  To start the\r\nscheduler with access to the private key, we must integrate with\r\nMarathon's DC/OS secret support:\r\n\r\n```javascript\r\n{\r\n  \"env\": {\r\n    \"DCOS_SERVICE_ACCOUNT_CREDENTIAL\": {\"secret\": \"serviceCredential\"}\r\n  },\r\n  \"secrets\": {\r\n    \"serviceCredential\": {\r\n      \"source\": <secret_name>\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis causes Marathon to launch the task with the\r\n`DCOS_SERVICE_ACCOUNT_CREDENTIAL` environment variable set to the\r\ncontents of the service account's private key.  The Mesos agent looks\r\nfor this environment variable and will inject it as the `secret` in\r\nthe scheduler's registration request.\r\n\r\n# HTTP API\r\n\r\nServices expose an HTTP API to support common operations.  There are\r\ntwo ways to access this API.\r\n\r\nThe first is from clients running outside of DC/OS, such as from your\r\nlocal machine.  These clients must authenticate.  For example:\r\n\r\n```bash\r\ncurl -H \"Authorization: token=$(dcos config show core.dcos_acs_token)\" \\\r\n    \"$(dcos config show core.dcos_url)/service/data-store/v1/plan/status\"\r\n```\r\n\r\nTo learn more about DC/OS Authentication, visit the\r\n[Managing\r\nAuthentication](https://dcos.io/docs/1.8/administration/id-and-access-\r\nmgt/managing-authentication/)\r\nsection of the DC/OS documentation.\r\n\r\nThe second way to access the HTTP API is from clients running inside\r\nthe\r\nDC/OS cluster.  These clients may bypass DC/OS authentication and\r\naccess the scheduler directly:\r\n\r\n```bash\r\ncurl \"data-store.marathon.mesos:<port>/v1/plan/status\"\r\n```\r\n\r\nWhere <port> is the port you gave to the `DefaultService` constructor\r\nin the [Define a Service](#define-a-service) section.\r\n\r\nSeveral API endpoints are listed in this tutorial and a full API is\r\nforthcoming.\r\n\r\n# Features at-a-glance\r\n\r\n- **Simple service definitions** - A simple, declarative API to\r\nsimplify resource offer configuration. [Learn\r\nmore](#simple-service-definitions).\r\n\r\n- **Multi-tier service support** - A scheduler created with the DC/OS\r\nStateful Services SDK is aware of dependencies between tiers of\r\nprocesses, enabling one-click install and safe failure recovery.\r\n[Learn more](#multi-tier-service-support).\r\n\r\n- **Deployment strategies** - The DC/OS Stateful Services SDK supports\r\ndiverse deployment strategies to fit the needs of schedulers. [Learn\r\nmore](#deployment-strategies).\r\n\r\n- **Configuration management** - The DC/OS Stateful Services SDK can\r\nread\r\nconfiguration from an external source, detect configuration\r\nmodifications, and control which configurations can be modified by the\r\nuser. [Learn more](#configuration-management).\r\n\r\n- **Interactive upgrade support** - You can pause, continue, or abort\r\nan upgrade interactively in order to upgrade tasks safely. [Learn\r\nmore](#interactive-upgrade-support).\r\n\r\n- **Fault tolerance** - Schedulers built with the SDK are\r\nautomatically configured to detect and\r\nrestart failed tasks on the same node (to preserve state) and, if the\r\nnode fails, restart it on a different node. [Learn\r\nmore](#fault-tolerance).\r\n\r\n- **Persistent volumes** - The SDK automatically configures your\r\nscheduler to\r\ncreate persistent volumes, enabling stateful tasks. [Learn\r\nmore](#persistent-volumes).\r\n\r\n# Features in-depth\r\n\r\n## Simple service definitions\r\nMesos offers resources to the scheduler for launching tasks rather\r\nthan launching tasks against a fixed API. Resource offers are more\r\npowerful and flexible than a declarative task launching API because\r\nschedulers can apply arbitrary logic to determine the set of resources\r\nthey consume.\r\n\r\nHowever, the majority of schedulers, especially schedulers for\r\nstateful services, only need a basic API. For these cases,\r\ndeclarative APIs are much simpler.  A simple scheduler only needs to\r\nrun N containers with M memory and C cpu, along with some set of\r\npersistent volumes, ports, etc.\r\n\r\nFor these cases, the DC/OS Stateful Services SDK provides a simple,\r\ndeclarative\r\nAPI built on top of Mesos resource offers. You define your service\r\ndeclaratively and the SDK manages installing and supervising the\r\nservice using resource offers from Mesos.\r\n\r\nYou can think of this as doing for stateful services what Marathon\r\ndoes for stateless services, while still allowing you to drop down to\r\nlower layers when you need more flexibility.\r\n\r\n## Multi-tier service support\r\nMany stateful services have multiple interdependent tiers,\r\neach running different processes.  For example, Apache HDFS is\r\ncomposed of NameNodes, JournalNodes, and DataNodes. JournalNodes start\r\nbefore NameNodes, which start before DataNodes.  To support one-click\r\ninstall for users, as well as safe recovery from failures, the\r\nscheduler must be aware of these dependencies.\r\n\r\n## Deployment strategies\r\nMany stateful services have strict deployment requirements.  Some,\r\nincluding, for instance Apache Cassandra, require nodes to be added to\r\nthe cluster one at at time.  Others may permit parallel deployment.\r\nThe DC/OS Stateful Services SDK includes deployment strategies\r\nthat support these different requirements.\r\n\r\n## Configuration management\r\n\r\nFor any framework, at least two components must be configured: the\r\nscheduler and the service.  Scheduler configuration includes things\r\nlike node count, deployment strategies, and security parameters.\r\nService configuration includes resource settings such memory, CPU, and\r\nports, as well as any configuration passed on to the underlying\r\nservice.\r\n\r\nThe DC/OS Stateful Services SDK includes logic for reading\r\nconfiguration from an external source (default: environment\r\nvariables), detecting changes to the configuration, and redeploying\r\nany affected tasks.  It also supports marking certain configuration\r\nparameters as \"unmodifiable,\" so that the user can't change them after\r\ninstall time.  For example, the disk size of permanent volumes cannot\r\nbe modified because volume size is static.\r\n\r\n## Interactive upgrade support\r\nTasks must often be updated one at a time, and often depend on certain\r\nadministrative tasks like backup/restore.  You can choose to pause,\r\ncontinue, or abort an upgrade interactively.\r\n\r\n## Fault tolerance\r\nThere are two component failures that any scheduler must defend\r\nagainst: the scheduler and the service. The SDK sets up fault\r\ntolerance for your scheduler automatically.\r\n\r\nThe SDK abstracts away task reconciliation with the Mesos master in\r\nthe case of task failure. In the case of service failure, the SDK\r\nconfigures your scheduler to restart the task on the same node or, if\r\nthe node has failed, restarts the task on a different node. The\r\nscheduler's persistent volume is reused on restart. A new persistent\r\nvolume is created when a node fails.\r\n\r\nThe mechanism to determine whether a task has permanently failed is time-based by default.\r\nWhen a task stays in a terminal state for some\r\nconfigurable duration, it is determined to have failed. The default\r\nduration is 20 minutes.  Once tasks have been determined to have\r\npermanently failed, a second configurable parameter determines how many \r\ndestructive task replacements may occur in a given time period.  By \r\ndefault, no more than one task may be destructively replaced in any 10 \r\nminute period.  Finally, automatic destructive recovery may be entirely\r\nsuppressed.\r\n\r\nConsider the simplest construct for the `DefaultScheduler`, in which\r\nautomatic destructive recovery configuration is exposed:\r\n\r\n```java\r\nDefaultScheduler create(\r\n        String frameworkName,\r\n        PlanManager deploymentPlanManager,\r\n        Optional<Integer> permanentFailureTimeoutSec,\r\n        Integer destructiveRecoveryDelaySec) {\r\n```\r\n\r\nThe `permanentFailureTimeoutSec` argument determines how long a task\r\nmust be in a terminal state before it is considered permanently failed.\r\nIf no value is present, automatic destructive recovery is turned off.\r\n\r\nThe `destructiveRecoveryDelaySec` argument determines how much time must\r\npass between destructive task replacement events.\r\n\r\nThe parameters above are one implementation of a more general\r\npermanent task recovery scheme.  The task recovery scheme has two major components: a _safety\r\nconstraint_ and a _performance constraint_.  In the example above, the\r\nduration that must be waited until a task is considered permanently \r\nfailed is the _safety constraint_. You can write a safety constraint that is more complex than a simple timeout.\r\n\r\nIn the example above, the recovery delay\r\nparameter is the _performance constraint_.  Even if a large number of tasks could be safely\r\ndestructively replaced, it could cause performance degradation\r\nas network traffic increases to reconstruct lost task state. In addition, you must be cautious when automating destructive recovery operations, so it is prudent to throttle the maximum rate of\r\ndestruction.\r\n\r\nRecovery, like deployment, is mediated by a plan. Use the endpoint below to view the status of the plan.\r\n```bash\r\nGET /v1/plans/recovery\r\n```\r\n\r\nIf no failures of any kind have occurred, you will see output similar to the snippet below.\r\n```json\r\n{\r\n\tphases: [{\r\n\t\tid: \"128d7df9-8605-4e1a-b98b-478821b1aeda\",\r\n\t\tname: \"recovery\",\r\n\t\tsteps: [],\r\n\t\tstatus: \"COMPLETE\"\r\n\t}],\r\n\terrors: [],\r\n\tstatus: \"COMPLETE\"\r\n}\r\n```\r\n\r\nIf a task has crashed and been recovered, you will see a plan similar\r\nto:\r\n```json\r\n{\r\n\tphases: [{\r\n\t\tid: \"88d944d3-4fc2-4605-889f-96b5429fb8af\",\r\n\t\tname: \"recovery\",\r\n\t\tsteps: [{\r\n\t\t\tid: \"ed4d2209-ab19-4242-b7da-d10cdf9e443b\",\r\n\t\t\tstatus: \"COMPLETE\",\r\n\t\t\tname: \"data-2\",\r\n\t\t\tmessage: \"org.apache.mesos.scheduler.recovery.DefaultRecoveryBlock: 'data-2 [ed4d2209-ab19-4242-b7da-d10cdf9e443b]' has status: 'COMPLETE'. RecoveryType: TRANSIENT\"\r\n\t\t}],\r\n\t\tstatus: \"COMPLETE\"\r\n\t}],\r\n\terrors: [],\r\n\tstatus: \"COMPLETE\"\r\n}\r\n```\r\n\r\nNote in particular the message that contains\r\n`RecoveryType: TRANSIENT` for the step named `data-2`.  This message indicates a\r\nrecovery from a temporary failure.  The task was able to be successfully\r\nrecovered in its previous location with all its old resources, including\r\npersistent volumes.  In the case of a permanent failure recovery, the\r\nmessage would instead contain `RecoveryType: PERMANENT`.\r\n\r\n## Persistent volumes\r\nSchedulers must create persistent volumes that will\r\nlive beyond the life a single task to tolerate failure, and they must\r\nreserve these volumes to prevent other frameworks from taking them.\r\nThe SDK configures persistent volumes for your scheduler automatically.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}