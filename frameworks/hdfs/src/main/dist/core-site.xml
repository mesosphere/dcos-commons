<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs</value>
    </property>
    <property>
        <name>hadoop.common.configuration.version</name>
        <value>{{HADOOP_COMMON_CONFIGURATION_VERSION}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>{{HADOOP_PROXYUSER_HUE_HOSTS}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>{{HADOOP_PROXYUSER_HUE_GROUPS}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>{{HADOOP_PROXYUSER_ROOT_HOSTS}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>{{HADOOP_PROXYUSER_ROOT_GROUPS}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.httpfs.groups</name>
        <value>{{HADOOP_PROXYUSER_HTTPFS_GROUPS}}</value>
    </property>
    <property>
        <name>hadoop.proxyuser.httpfs.hosts</name>
        <value>{{HADOOP_PROXYUSER_HTTPFS_HOSTS}}</value>
    </property>
    <property>
        <name>ha.zookeeper.parent-znode</name>
        <value>{{SERVICE_ZK_ROOT}}/hadoop-ha</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>{{IPC_CLIENT_CONNECT_MAX_RETRIES}}</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>{{HADOOP_TMP_DIR}}</value>
    </property>
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>{{HADOOP_HTTP_FILTER_INITIALIZER}}</value>
    </property>
    {{^SECURITY_KERBEROS_ENABLED}}
    <property>
        <name>hadoop.rpc.protection</name>
        <value>{{HADOOP_RPC_PROTECTION}}</value>
    </property>
    {{/SECURITY_KERBEROS_ENABLED}}
    <property>
        <name>hadoop.work.around.non.threadsafe.getpwuid</name>
        <value>{{HADOOP_WORK_AROUND_NON_THREADSAFE_GETPWUID}}</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>{{IO_FILE_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>io.bytes.per.checksum</name>
        <value>{{IO_BYTES_PER_CHECKSUM}}</value>
    </property>
    <property>
        <name>io.skip.checksum.errors</name>
        <value>{{IO_SKIP_CHECKSUM_ERRORS}}</value>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>{{IO_COMPRESSION_CODECS}}</value>
    </property>
    <property>
        <name>io.compression.codec.bzip2.library</name>
        <value>{{IO_COMPRESSION_CODEC_BZIP2_LIBRARY}}</value>
    </property>
    <property>
        <name>io.serializations</name>
        <value>{{IO_SERIALIZATIONS}}</value>
    </property>
    <property>
        <name>io.seqfile.local.dir</name>
        <value>{{IO_SEQFILE_LOCAL_DIR}}</value>
    </property>
    <property>
        <name>io.map.index.skip</name>
        <value>{{IO_MAP_INDEX_SKIP}}</value>
    </property>
    <property>
        <name>io.map.index.interval</name>
        <value>{{IO_MAP_INDEX_INTERVAL}}</value>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>{{FS_TRASH_INTERVAL}}</value>
    </property>
    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>{{FS_TRASH_CHECKPOINT_INTERVAL}}</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.file.impl</name>
        <value>{{FS_ABSTRACTFILESYSTEM_FILE_IMPL}}</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.har.impl</name>
        <value>{{FS_ABSTRACTFILESYSTEM_HAR_IMPL}}</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.hdfs.impl</name>
        <value>{{FS_ABSTRACTFILESYSTEM_HDFS_IMPL}}</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.viewfs.impl</name>
        <value>{{FS_ABSTRACTFILESYSTEM_VIEWFS_IMPL}}</value>
    </property>
    <property>
        <name>fs.ftp.host</name>
        <value>{{FS_FTP_HOST}}</value>
    </property>
    <property>
        <name>fs.ftp.host.port</name>
        <value>{{FS_FTP_HOST_PORT}}</value>
    </property>
    <property>
        <name>fs.df.interval</name>
        <value>{{FS_DF_INTERVAL}}</value>
    </property>
    <property>
        <name>fs.du.interval</name>
        <value>{{FS_DU_INTERVAL}}</value>
    </property>
    <property>
        <name>fs.s3.block.size</name>
        <value>{{FS_S3_BLOCK_SIZE}}</value>
    </property>
    <property>
        <name>fs.s3.buffer.dir</name>
        <value>{{FS_S3_BUFFER_DIR}}</value>
    </property>
    <property>
        <name>fs.s3.maxRetries</name>
        <value>{{FS_S3_MAXRETRIES}}</value>
    </property>
    <property>
        <name>fs.s3.sleepTimeSeconds</name>
        <value>{{FS_S3_SLEEP_TIME_SECONDS}}</value>
    </property>
    <property>
        <name>fs.swift.impl</name>
        <value>{{FS_SWIFT_IMPL}}</value>
    </property>
    <property>
        <name>fs.automatic.close</name>
        <value>{{FS_AUTOMATIC_CLOSE}}</value>
    </property>
    <property>
        <name>fs.s3n.block.size</name>
        <value>{{FS_S3N_BLOCK_SIZE}}</value>
    </property>
    <property>
        <name>fs.s3n.multipart.uploads.enabled</name>
        <value>{{FS_S3N_MULTIPART_UPLOADS_ENABLED}}</value>
    </property>
    <property>
        <name>fs.s3n.multipart.uploads.block.size</name>
        <value>{{FS_S3N_MULTIPART_UPLOADS_BLOCK_SIZE}}</value>
    </property>
    <property>
        <name>fs.s3n.multipart.copy.block.size</name>
        <value>{{FS_S3N_MULTIPART_COPY_BLOCK_SIZE}}</value>
    </property>
    <property>
        <name>fs.s3n.server-side-encryption-algorithm</name>
        <value>{{FS_S3N_SERVER_SIDE_ENCRPYTION_ALGORITHM}}</value>
    </property>
    <property>
        <name>fs.s3a.access.key</name>
        <value>{{FS_S3N_ACCESS_KEY}}</value>
    </property>
    <property>
        <name>fs.s3a.secret.key</name>
        <value>{{FS_S3N_SECRET_KEY}}</value>
    </property>
    <property>
        <name>fs.s3a.connection.maximum</name>
        <value>{{FS_S3N_CONNECTION_MAXIMUM}}</value>
    </property>
    <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>{{FS_S3N_CONNECTION_SSL_ENABLED}}</value>
    </property>
    <property>
        <name>fs.s3a.attempts.maximum</name>
        <value>{{FS_S3N_ATTEMPTS_MAXIMUM}}</value>
    </property>
    <property>
        <name>fs.s3a.connection.timeout</name>
        <value>{{FS_S3N_CONNECTION_TIMEOUT}}</value>
    </property>
    <property>
        <name>fs.s3a.paging.maximum</name>
        <value>{{FS_S3N_PAGING_MAXIMUM}}</value>
    </property>
    <property>
        <name>fs.s3a.multipart.size</name>
        <value>{{FS_S3N_MULTIPART_SIZE}}</value>
    </property>
    <property>
        <name>fs.s3a.multipart.threshold</name>
        <value>{{FS_S3N_MULTIPART_THRESHOLD}}</value>
    </property>
    <property>
        <name>fs.s3a.acl.default</name>
        <value>{{FS_S3N_ACL_DEFAULT}}</value>
    </property>
    <property>
        <name>fs.s3a.multipart.purge</name>
        <value>{{FS_S3N_MULTIPART_PURGE}}</value>
    </property>
    <property>
        <name>fs.s3a.multipart.purge.age</name>
        <value>{{FS_S3N_MULTIPART_PURGE_AGE}}</value>
    </property>
    <property>
        <name>fs.s3a.buffer.dir</name>
        <value>{{FS_S3N_BUFFER_DIR}}</value>
    </property>
    <property>
        <name>fs.s3a.impl</name>
        <value>{{FS_S3N_IMPL}}</value>
    </property>
    <property>
        <name>io.seqfile.compress.blocksize</name>
        <value>{{IO_SEQFILE_COMPRESS_BLOCKSIZE}}</value>
    </property>
    <property>
        <name>io.seqfile.lazydecompress</name>
        <value>{{IO_SEQFILE_LAZYDECOMPRESS}}</value>
    </property>
    <property>
        <name>io.seqfile.sorter.recordlimit</name>
        <value>{{IO_SEQFILE_SORTER_RECORDLIMIT}}</value>
    </property>
    <property>
        <name>io.mapfile.bloom.size</name>
        <value>{{IO_SEQFILE_BLOOM_SIZE}}</value>
    </property>
    <property>
        <name>io.mapfile.bloom.error.rate</name>
        <value>{{IO_SEQFILE_BLOOM_ERROR_RATE}}</value>
    </property>
    <property>
        <name>hadoop.util.hash.type</name>
        <value>{{HADOOP_UTIL_HASH_TYPE}}</value>
    </property>
    <property>
        <name>ipc.client.idlethreshold</name>
        <value>{{IPC_CLIENT_IDLETHRESHOLD}}</value>
    </property>
    <property>
        <name>ipc.client.kill.max</name>
        <value>{{IPC_CLIENT_KILL_MAX}}</value>
    </property>
    <property>
        <name>ipc.client.connection.maxidletime</name>
        <value>{{IPC_CLIENT_CONNECTION_MAXIDLETIME}}</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>{{IPC_CLIENT_CONNECT_MAX_RETRIES}}</value>
    </property>
    <property>
        <name>ipc.client.connect.retry.interval</name>
        <value>{{IPC_CLIENT_CONNECT_RETRY_INTERVAL}}</value>
    </property>
    <property>
        <name>ipc.client.connect.timeout</name>
        <value>{{IPC_CLIENT_CONNECT_TIMEOUT}}</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries.on.timeouts</name>
        <value>{{IPC_CLIENT_CONNECT_MAX_RETRIES_ON_TIMEOUTS}}</value>
    </property>
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>{{IPC_SERVER_LISTEN_QUEUE_SIZE}}</value>
    </property>
    <property>
        <name>hadoop.rpc.socket.factory.class.default</name>
        <value>{{HADOOP_RPC_SOCKET_FACTORY_CLASS_DEFAULT}}</value>
    </property>
    <property>
        <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
        <value>{{HADOOP_RPC_SOCKET_FACTORY_CLASS_CLIENT_PROTOCOL}}</value>
    </property>
    <property>
        <name>hadoop.socks.server</name>
        <value>{{HADOOP_SOCKS_SERVER}}</value>
    </property>
    {{#PLACEMENT_REFERENCED_ZONE}}
    <property>
        <name>net.topology.node.switch.mapping.impl</name>
        <value>org.apache.hadoop.net.ScriptBasedMapping</value>
    </property>
    <property>
        <name>net.topology.script.file.name</name>
        <value>{{HDFS_VERSION}}/etc/hadoop/zone-resolver.sh</value>
    </property>
    <property>
        <name>net.topology.script.number.args</name>
        <value>1</value>
    </property>
    {{/PLACEMENT_REFERENCED_ZONE}}
    <property>
        <name>file.stream-buffer-size</name>
        <value>{{FILE_STREAM_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>file.bytes-per-checksum</name>
        <value>{{FILE_BYTES_PER_CHECKSUM}}</value>
    </property>
    <property>
        <name>file.client-write-packet-size</name>
        <value>{{FILE_CLIENT_WRITE_PACKET_SIZE}}</value>
    </property>
    <property>
        <name>file.blocksize</name>
        <value>{{FILE_BLOCKSIZE}}</value>
    </property>
    <property>
        <name>file.replication</name>
        <value>{{FILE_REPLICATION}}</value>
    </property>
    <property>
        <name>s3.stream-buffer-size</name>
        <value>{{S3_STREAM_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>s3.bytes-per-checksum</name>
        <value>{{S3_BYTES_PER_CHECKSUM}}</value>
    </property>
    <property>
        <name>s3.client-write-packet-size</name>
        <value>{{S3_CLIENT_WRITE_PACKET_SIZE}}</value>
    </property>
    <property>
        <name>s3.blocksize</name>
        <value>{{S3_BLOCKSIZE}}</value>
    </property>
    <property>
        <name>s3.replication</name>
        <value>{{S3_REPLICATION}}</value>
    </property>
    <property>
        <name>s3native.stream-buffer-size</name>
        <value>{{S3NATIVE_STREAM_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>s3native.bytes-per-checksum</name>
        <value>{{S3NATIVE_BYTES_PER_CHECKSUM}}</value>
    </property>
    <property>
        <name>s3native.client-write-packet-size</name>
        <value>{{S3NATIVE_CLIENT_WRITE_PACKET_SIZE}}</value>
    </property>
    <property>
        <name>s3native.blocksize</name>
        <value>{{S3NATIVE_BLOCKSIZE}}</value>
    </property>
    <property>
        <name>s3native.replication</name>
        <value>{{S3NATIVE_REPLICATION}}</value>
    </property>
    <property>
        <name>ftp.stream-buffer-size</name>
        <value>{{FTP_STREAM_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>ftp.bytes-per-checksum</name>
        <value>{{FTP_BYTES_PER_CHECKSUM}}</value>
    </property>
    <property>
        <name>ftp.client-write-packet-size</name>
        <value>{{FTP_CLIENT_WRITE_PACKET_SIZE}}</value>
    </property>
    <property>
        <name>ftp.blocksize</name>
        <value>{{FTP_BLOCKSIZE}}</value>
    </property>
    <property>
        <name>ftp.replication</name>
        <value>{{FTP_REPLICATION}}</value>
    </property>
    <property>
        <name>tfile.io.chunk.size</name>
        <value>{{TFILE_IO_CHUNK_SIZE}}</value>
    </property>
    <property>
        <name>tfile.fs.output.buffer.size</name>
        <value>{{TFILE_IO_OUTPUT_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>tfile.fs.input.buffer.size</name>
        <value>{{TFILE_FS_INPUT_BUFFER_SIZE}}</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>{{DFS_HA_FENCING_SSH_CONNECT_TIMEOUT}}</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>{{DFS_HA_FENCING_SSH_PRIVATE_KEY_FILES}}</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>{{HADOOP_HTTP_STATICUSER_USER}}</value>
    </property>
    <property>
        <name>ha.zookeeper.session-timeout.ms</name>
        <value>{{HA_ZOOKEEPER_SESSION_TIMEOUT_MS}}</value>
    </property>
    <property>
        <name>hadoop.ssl.keystores.factory.class</name>
        <value>{{HADOOP_SSL_KEYSTORES_FACTORY_CLASS}}</value>
    </property>
    <property>
        <name>hadoop.ssl.require.client.cert</name>
        <value>{{HADOOP_SSL_REQUIRE_CLIENT_CERT}}</value>
    </property>
    <property>
        <name>hadoop.jetty.logs.serve.aliases</name>
        <value>{{HADOOP_JETTY_LOGS_SERVE_ALIASES}}</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>{{FS_PERMISSIONS_UMASK_MODE}}</value>
    </property>
    <property>
        <name>ha.health-monitor.connect-retry-interval.ms</name>
        <value>{{HA_HEALTH_MONITOR_CONNECT_RETRY_INTERVAL_MS}}</value>
    </property>
    <property>
        <name>ha.health-monitor.check-interval.ms</name>
        <value>{{HA_HEALTH_MONITOR_CHECK_INTERVAL_MS}}</value>
    </property>
    <property>
        <name>ha.health-monitor.sleep-after-disconnect.ms</name>
        <value>{{HA_HEALTH_MONITOR_SLEEP_AFTER_DISCONNECT_MS}}</value>
    </property>
    <property>
        <name>ha.health-monitor.rpc-timeout.ms</name>
        <value>{{HA_HEALTH_MONITOR_RPC_TIMEOUT_MS}}</value>
    </property>
    <property>
        <name>ha.failover-controller.new-active.rpc-timeout.ms</name>
        <value>{{HA_FAILOVER_CONTROLLER_NEW_ACTIVE_RPC_TIMEOUT_MS}}</value>
    </property>
    <property>
        <name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name>
        <value>{{HA_FAILOVER_CONTROLLER_GRACEFUL_FENCE_RPC_TIMEOUT_MS}}</value>
    </property>
    <property>
        <name>ha.failover-controller.graceful-fence.connection.retries</name>
        <value>{{HA_FAILOVER_CONTROLLER_GRACEFUL_FENCE_CONNECTION_RETRIES}}</value>
    </property>
    <property>
        <name>ha.failover-controller.cli-check.rpc-timeout.ms</name>
        <value>{{HA_FAILOVER_CONTROLLER_CLI_CHECK_RPC_TIMEOUT_MS}}</value>
    </property>
    <property>
        <name>ipc.client.fallback-to-simple-auth-allowed</name>
        <value>{{IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED}}</value>
    </property>
    <property>
        <name>fs.client.resolve.remote.symlinks</name>
        <value>{{FS_CLIENT_RESOLVE_REMOTE_SYMLINKS}}</value>
    </property>
    <property>
        <name>nfs.exports.allowed.hosts</name>
        <value>{{NFS_EXPORTS_ALLOWED_HOSTS}}</value>
    </property>
    <property>
        <name>hadoop.user.group.static.mapping.overrides</name>
        <value>{{HADOOP_USER_GROUP_STATIC_MAPPING_OVERRIDES}}</value>
    </property>
    <property>
        <name>rpc.metrics.quantile.enable</name>
        <value>{{RPC_METRICS_QUANTILE_ENABLE}}</value>
    </property>
    <property>
        <name>rpc.metrics.percentiles.intervals</name>
        <value>{{RPC_METRICS_PERCENTILES_INTERVALS}}</value>
    </property>
    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>{{HADOOP_HTTP_AUTHENTICATION_SIMPLE_ANONYMOUS_ALLOWED}}</value>
    </property>

    {{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}
    <property>
        <name>hadoop.ssl.enabled.protocols</name>
        <value>TLSv1.2</value>
    </property>
    <property>
        <name>hadoop.ssl.hostname.verifier</name>
        <value>ALLOW_ALL</value>
    </property>
    {{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}
    {{#SECURITY_KERBEROS_ENABLED}}
    <property> <!-- The ZKFC nodes use this property to verify they are connecting to the namenode with the expected principal. -->
        <name>hadoop.security.service.user.name.key.pattern</name>
        <value>{{SECURITY_KERBEROS_PRIMARY}}/*@{{SECURITY_KERBEROS_REALM}}</value>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>
    <property>
        <name>hadoop.rpc.protection</name>
        <value>privacy</value>
    </property>
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            {{{DECODED_AUTH_TO_LOCAL}}}
        </value>
    </property>

    <property>
        <name>hadoop.http.authentication.type</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>{{HADOOP_HTTP_AUTHENTICATION_TOKEN_VALIDITY}}</value>
    </property>
    <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value>{{FRAMEWORK_HOST}}</value>
    </property>
    <property>
        <name>hadoop.http.authentication.kerberos.principal</name>
        <value>{{SECURITY_KERBEROS_PRIMARY_HTTP}}/{{SCHEDULER_API_HOSTNAME}}@{{SECURITY_KERBEROS_REALM}}</value>
    </property>
    <property>
        <name>hadoop.http.authentication.kerberos.keytab</name>
        <value>hdfs.keytab</value>
    </property>


    <property>
        <name>hadoop.kerberos.kinit.command</name>
        <value>{{HADOOP_KERBEROS_KINIT_COMMAND}}</value>
    </property>
    <property>
        <name>hadoop.security.instrumentation.requires.admin</name>
        <value>{{HADOOP_SECURITY_INSTRUMENTATION_REQUIRES_ADMIN}}</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping</name>
        <value>{{HADOOP_SECURITY_GROUP_MAPPING}}</value>
    </property>
    <property>
        <name>hadoop.security.groups.cache.secs</name>
        <value>{{HADOOP_SECURITY_GROUPS_CACHE_SECS}}</value>
    </property>
    <property>
        <name>hadoop.security.groups.negative-cache.secs</name>
        <value>{{HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS}}</value>
    </property>
    <property>
        <name>hadoop.security.groups.cache.warn.after.ms</name>
        <value>{{HADOOP_SECURITY_GROUPS_CACHE_WARN_AFTER_MS}}</value>
    </property>
    <property>
        <name>hadoop.security.service.user.name.key</name>
        <value>{{HADOOP_SECURITY_SERVICE_USER_NAME_KEY}}</value>
    </property>
    <property>
        <name>hadoop.security.uid.cache.secs</name>
        <value>{{HADOOP_SECURITY_UID_CACHE_SECS}}</value>
    </property>
    <property>
        <name>hadoop.security.saslproperties.resolver.class</name>
        <value>{{HADOOP_SECURITY_SASLPROPERTIES_RESOLVER_CLASS}}</value>
    </property>
    <property>
        <name>hadoop.security.impersonation.provider.class</name>
        <value>{{HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS}}</value>
    </property>
    {{/SECURITY_KERBEROS_ENABLED}}
</configuration>
